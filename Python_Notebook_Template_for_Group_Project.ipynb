{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8D2mabhtCeh"
      },
      "source": [
        "# Title of Your Group Project\n",
        "\n",
        "This Python notebook serves as a template for your group project for the course \"Modeling in Cognitive Science\".\n",
        "\n",
        "This is the practical part of the group project where you get to implement the computational modeling workflow. In this part, you are expected to:\n",
        "\n",
        "\n",
        "*   Implement at least two computational models relevant for your hypothesis. *(3 points)*\n",
        "*   Simulate behavior from the two models. *(3 points)*\n",
        "*   Implement a procedure for fitting the models to data. *(4 points)*\n",
        "*   Implement a procedure for parameter recovery. *(5 points)*\n",
        "*   (Implement a procedure for model recovery.) *(optional; 2 bonus points)*\n",
        "*   Implement a model comparison. *(5 points)*.\n",
        "\n",
        "You can gain a total of 20 points for the practical part of the group project.\n",
        "\n",
        "**Note:** *Some of the exercises below (e.g. Model Simulation) rely on code from previous exercises (e.g., Model Implementation). In such cases, you are encouraged to rely on functions implemented for previous exercises. That is, you don't have to produce redundant code.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HceMyA8DtIZ3"
      },
      "source": [
        "## Model Implementation *(3 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Implement and simulate data from two* models that are suitable to test your hypothesis. *(3 points)*\n",
        "\n",
        "<font size=2>*You may implement more than two models if you wish. However, two models are sufficient for this group project.</font>\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "isMmbQsKwZ_z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# environment of the experiment\n",
        "class TowStepEnv:\n",
        "    # TODO \n",
        "    # - randome seed\n",
        "    action_space = [0, 1]\n",
        "    state_space = [0, 1, 2]\n",
        "    def __init__(self):\n",
        "        self.state = 0\n",
        "        # self.action_space = [0, 1]\n",
        "        # self.state_space = [0, 1, 2]\n",
        "        self.transition_prob = 0.7\n",
        "        self.reward = 1\n",
        "        self.terminal = False\n",
        "        self.info = {}\n",
        "        \n",
        "        # matrix of transition probabilities\n",
        "        # 0(action left) -> [0(stay in 0), p(go to 1), 1-p(go to 2)]\n",
        "        # 1(action right) -> [0(stay in 0), 1-p(go to 1), p(go to 2)]\n",
        "        self.stage_1_transition_matrix = np.array([[0, self.transition_prob, 1 - self.transition_prob], # action left\n",
        "                                           [0, 1 - self.transition_prob, self.transition_prob]]) # action right\n",
        "        \n",
        "        # self.seed = 0\n",
        "        # np.random.seed(self.seed)\n",
        "        self.min_reward_prob = 0.25\n",
        "        self.max_reward_prob = 0.75\n",
        "        # matrix of reward probabilities\n",
        "        # 0(state 0) -> [0 (left), 0(right)]\n",
        "        # 1(state 1) -> [p1 (left), p2(right)]\n",
        "        # 2(state 2) -> [p3 (left), p4(right)]\n",
        "        p1 = np.random.uniform(self.min_reward_prob, self.max_reward_prob) \n",
        "        p2 = np.random.uniform(self.min_reward_prob, self.max_reward_prob)\n",
        "        p3 = np.random.uniform(self.min_reward_prob, self.max_reward_prob)\n",
        "        p4 = np.random.uniform(self.min_reward_prob, self.max_reward_prob)\n",
        "        # p1 = 0.75\n",
        "        # p2 = 0.75\n",
        "        # p3 = 0.25\n",
        "        # p4 = 0.25\n",
        "\n",
        "        self.reward_prob_matrix = np.array([[0, 0], # first stage (state 0) for both actions\n",
        "                                            [p1, p2], # second stage (state 1) for both actions\n",
        "                                            [p3, p4]]) # second stage (state 2) for both actions\n",
        "        \n",
        "        # 1 -> fixed reward prob.\n",
        "        # 0 -> reward prob. can be changed a long the trails \n",
        "        self.fixed_reward_prob_matrix = np.array([[1, 1],\n",
        "                                            [0, 0],\n",
        "                                            [0, 0]])\n",
        "        \n",
        "    \n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "        self.terminal = False\n",
        "        self.info = {}\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.terminal:\n",
        "            raise ValueError(\"Episode has already terminated\")\n",
        "        if action not in self.action_space:\n",
        "            raise ValueError(f\"The action: {action} is not valid, action space: {self.action_space}\")\n",
        "\n",
        "        # if in stage 1\n",
        "        if self.state == 0:\n",
        "            reward = self.reward_function(self.state, action) # reward will be 0\n",
        "            self.state = np.random.choice(self.state_space, p=self.stage_1_transition_matrix[action])\n",
        "\n",
        "            self.info[\"common_transition\"] = self.is_common_state(self.state, action)\n",
        "            self.info[\"state_transition_to\"] = self.state\n",
        "            self.info[\"reward_stage_1\"] = reward\n",
        "            self.info[\"action_stage_1\"] = action\n",
        "            # self.info[\"reward_probabilities_stage_1\"] = self.reward_prob_matrix.flatten()\n",
        "        \n",
        "        # if in stage 2\n",
        "        elif self.state in [1,2]:\n",
        "            reward = self.reward_function(self.state, action)\n",
        "            self.terminal = True\n",
        "            self.info[\"reward_stage_2\"] = reward\n",
        "            self.info[\"action_stage_2\"] = action\n",
        "            # self.info[\"reward_probabilities\"] = self.reward_prob_matrix.flatten()\n",
        "            self.info[\"reward_probabilities\"] = self.reward_prob_matrix.flatten()\n",
        "\n",
        "        \n",
        "        else:\n",
        "            raise ValueError(f\"state:{self.state} is an invalid state, state space: {self.state_space}\")\n",
        "        \n",
        "        \n",
        "        return self.state, reward, self.terminal, self.info\n",
        "    \n",
        "    def reward_function(self, state, action):\n",
        "        if action not in self.action_space:\n",
        "            raise ValueError(f\"The action: {action} is not valid, action space: {self.action_space}\")\n",
        "        if state not in self.state_space:\n",
        "            raise ValueError(f\"state:{state} is an invalid state, state space: {self.state_space}\")\n",
        "        \n",
        "        # give a reward according to the probability of getting a reward\n",
        "        # for the action taken in the state ( state-action pair )\n",
        "        reward = np.random.uniform() < self.reward_prob_matrix[state][action]\n",
        "        # scale the reward for a costume reward value equal to self.reward\n",
        "        # makes no difference in case self.reward = 1\n",
        "        reward = reward * self.reward\n",
        "        return reward\n",
        "    \n",
        "    def state_transition_function(self, state, action):\n",
        "        if action not in self.action_space:\n",
        "            raise ValueError(f\"The action: {action} is not valid, action space: {self.action_space}\")\n",
        "        \n",
        "        new_state = None\n",
        "        terminal = False\n",
        "        if state == 0:\n",
        "            new_state = np.random.choice(self.state_space, p=self.stage_1_transition_matrix[action])\n",
        "        elif state in [1,2]:\n",
        "            terminal = True\n",
        "        else:\n",
        "            raise ValueError(f\"state:{state} is an invalid state, state space: {self.state_space}\")\n",
        "        \n",
        "        return new_state, terminal\n",
        "\n",
        "    def is_common_state(self, state, action):\n",
        "        if action not in self.action_space:\n",
        "            raise ValueError(f\"The action: {action} is not valid, action space: {self.action_space}\")\n",
        "        if state not in self.state_space:\n",
        "            raise ValueError(f\"state:{state} is an invalid state, state space: {self.state_space}\")\n",
        "        \n",
        "        # return self.stage_1_transition_matrix[action, state] >= 0.5\n",
        "        return self.stage_1_transition_matrix[action, state] == np.max(self.stage_1_transition_matrix[action])\n",
        "    \n",
        "    def set_reward_probabilities(self, reward_prob_matrix):\n",
        "        if reward_prob_matrix.shape != self.reward_prob_matrix.shape:\n",
        "            raise ValueError(f\"reward_prob_matrix shape: {reward_prob_matrix.shape} is not valid, shape should be {self.reward_prob_matrix.shape}\")\n",
        "        # clip the reward probabilities to be between min_reward_prob and max_reward_prob\n",
        "        reward_prob_matrix = np.clip(reward_prob_matrix, self.min_reward_prob, self.max_reward_prob)\n",
        "        \n",
        "        # update the reward_prob_matrix\n",
        "        # if the reward_prob_matrix is fixed -> do not update it, else update it with from the new reward_prob_matrix\n",
        "        self.reward_prob_matrix = np.where(self.fixed_reward_prob_matrix, self.reward_prob_matrix, reward_prob_matrix)\n",
        "        return self.reward_prob_matrix\n",
        "\n",
        "    def set_seed(self, seed):\n",
        "        pass\n",
        "\n",
        "    def plot(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [],
      "source": [
        "# agent / models\n",
        "\n",
        "class RandomAgent:\n",
        "    def __init__(self, action_space, state_space, alpha=0.1, gamma=0.9):\n",
        "        # the state space can be infered but here it is given for simplicity\n",
        "        self.action_space = action_space\n",
        "        self.state_space = state_space\n",
        "        self.q_table = np.zeros((len(self.state_space), len(self.action_space)))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def policy(self, state):\n",
        "        return np.random.choice(self.action_space)\n",
        "    \n",
        "    def update_q_table_sarsa(self, state, action, reward, next_state, terminal):\n",
        "        if state not in self.state_space or next_state not in self.state_space:\n",
        "            raise ValueError(f\"state:{state} is an invalid state, state space: {self.state_space}\")\n",
        "        if action not in self.action_space:\n",
        "            raise ValueError(f\"The action: {action} is not valid, action space: {self.action_space}\")\n",
        "        \n",
        "        if terminal:\n",
        "            self.q_table[state, action] += self.alpha * (reward - self.q_table[state, action])\n",
        "        else:\n",
        "            next_action = self.policy(next_state)\n",
        "            self.q_table[state, action] += self.alpha * (reward + self.gamma * self.q_table[next_state, next_action] - self.q_table[state, action])\n",
        "        return self.q_table\n",
        "    \n",
        "    def update_beliefs(self, state, action, reward, next_state, terminal):\n",
        "        self.update_q_table_sarsa(self, state, action, reward, next_state, terminal)\n",
        "    \n",
        "    def reset(self):\n",
        "        pass\n",
        "# -------------------------------------------------------------------------------------------------------------\n",
        "# -------------------------------------------------------------------------------------------------------------\n",
        "# -------------------------------------------------------------------------------------------------------------\n",
        "        \n",
        "class AgentModelFree:\n",
        "    def __init__(self, action_space, state_space, alpha=0.1, gamma=0.9, beta=1.0):\n",
        "        # the state space can be infered but here it is given for simplicity\n",
        "        self.action_space = action_space\n",
        "        self.state_space = state_space\n",
        "        self.alpha = alpha # Learning rate\n",
        "        self.gamma = gamma # Discount factor\n",
        "        self.beta = beta  # Temperature parameter for softmax policy\n",
        "        self.epsilon = 0.2  # Epsilon for epsilon-greedy policy\n",
        "        self.q_table = np.zeros((len(self.state_space), len(self.action_space)))\n",
        "\n",
        "    def softmax(self, arr, beta):\n",
        "        e_x = np.exp(beta * (arr - np.max(arr)))  # subtract max value to prevent overflow\n",
        "        return e_x / e_x.sum(axis=0)  # axis=0 for column-wise operation if arr is 2D, otherwise it's not needed\n",
        "\n",
        "    def policy(self, state, beta=None, epsilon=None,method=\"softmax\"):\n",
        "        q_values = self.q_table[state, :]\n",
        "        beta = self.beta if beta is None else beta\n",
        "        epsilon = self.epsilon if epsilon is None else epsilon\n",
        "        # calculate the probability of each action in the state with softmax\n",
        "        if method == \"softmax\":\n",
        "            action_probabilities = self.softmax(q_values, beta)\n",
        "            action = np.random.choice(self.action_space, p=action_probabilities)\n",
        "        \n",
        "        # with epsilon gready policy\n",
        "        else:\n",
        "            if np.random.uniform() < epsilon:\n",
        "                action = np.random.choice(self.action_space)\n",
        "            else:\n",
        "                action = np.argmax(q_values)\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def update_q_table_sarsa(self, state, action, reward, next_state, terminal):\n",
        "        if state not in self.state_space or next_state not in self.state_space:\n",
        "            raise ValueError(f\"state:{state} is an invalid state, state space: {self.state_space}\")\n",
        "        if action not in self.action_space:\n",
        "            raise ValueError(f\"The action: {action} is not valid, action space: {self.action_space}\")\n",
        "        \n",
        "        next_action = self.policy(next_state)\n",
        "        self.q_table[state, action] += self.alpha * self.reward_prediction_error(state, action, reward, next_state, next_action, terminal)\n",
        "        \n",
        "        return self.q_table\n",
        "\n",
        "    def reward_prediction_error(self, state, action, reward, next_state, next_action, terminal):\n",
        "        if terminal:\n",
        "            return reward - self.q_table[state, action]\n",
        "        return reward + self.gamma * self.q_table[next_state, next_action] - self.q_table[state, action]\n",
        "    \n",
        "    def update_beliefs(self, state, action, reward, next_state, terminal):\n",
        "        self.update_q_table_sarsa(state, action, reward, next_state, terminal)\n",
        "    \n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------\n",
        "# -------------------------------------------------------------------------------------------------------------\n",
        "# -------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "class AgentModelBased:\n",
        "    def __init__(self, action_space, state_space, alpha=0.1, gamma=0.9, beta=1.0, epsilon=0.2):\n",
        "        self.action_space = action_space\n",
        "        self.state_space = state_space\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.beta = beta  # Temperature parameter for softmax policy\n",
        "        self.epsilon = 0.2  # Epsilon for epsilon-greedy policy\n",
        "        self.q_table = np.zeros((len(state_space), len(action_space)))\n",
        "\n",
        "        # Initialize transition model as a 3D numpy array\n",
        "        # Dimensions: [current_state, action, next_state]\n",
        "        # For simplicity, initializing all transitions as equally likely\n",
        "        self.transition_model = np.zeros((len(state_space),\n",
        "                                         len(action_space),\n",
        "                                         len(state_space)))\n",
        "        self.transition_counts = np.zeros((len(state_space),\n",
        "                                            len(action_space),\n",
        "                                            len(state_space)))\n",
        "\n",
        "    def softmax(self, arr, beta):\n",
        "        e_x = np.exp(beta * (arr - np.max(arr)))  # subtract max value to prevent overflow\n",
        "        return e_x / e_x.sum(axis=0)  # axis=0 for column-wise operation if arr is 2D, otherwise it's not needed\n",
        "\n",
        "    def policy(self, state, beta=None, epsilon=None,method=\"softmax\"):\n",
        "        q_values = self.q_table[state, :]\n",
        "        beta = self.beta if beta is None else beta\n",
        "        epsilon = self.epsilon if epsilon is None else epsilon\n",
        "        # calculate the probability of each action in the state with softmax\n",
        "        if method == \"softmax\":\n",
        "            action_probabilities = self.softmax(q_values, beta)\n",
        "            action = np.random.choice(self.action_space, p=action_probabilities)\n",
        "        \n",
        "        # with epsilon gready policy\n",
        "        else:\n",
        "            if np.random.uniform() < epsilon:\n",
        "                action = np.random.choice(self.action_space)\n",
        "            else:\n",
        "                action = np.argmax(q_values)\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def update_transition_model(self, current_state, action, next_state, terminal):\n",
        "        # Simple counting method to update transition probabilities\n",
        "        # TODO - Implement more sophisticated methods like Bayesian updating\n",
        "        #      - at least insure no 0 probabilities for stage 1 to stage 2 transitions\n",
        "        if terminal:\n",
        "            return\n",
        "        # Increment the count for the observed transition\n",
        "        self.transition_counts[current_state, action, next_state] += 1\n",
        "\n",
        "        # Normalize the transition probabilities for the current state-action pair\n",
        "        total_transitions = self.transition_counts[current_state, action, :].sum()\n",
        "        self.transition_model[current_state, action, :] = self.transition_counts[current_state, action, :] / total_transitions\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state, terminal):\n",
        "        # Update Q-table using the transition model\n",
        "        if terminal: # -> second stage -> update with TD\n",
        "            self.q_table[state, action] += self.alpha * self.reward_prediction_error(state, action, reward, next_state, terminal)\n",
        "\n",
        "        else:\n",
        "            # self.q_table[state, action] +=  self.transition_model[state, action, next_state] * self.alpha * self.reward_prediction_error(state, action, reward, next_state, terminal)\n",
        "            # self.q_table[state, action] =  self.transition_model[state, action, next_state] * self.alpha * self.reward_prediction_error(state, action, reward, next_state, terminal)\n",
        "\n",
        "            self.q_table[state, action] = np.sum([self.transition_model[state, action, possible_state] * np.max(\n",
        "                [self.q_table[possible_state, action] + self.alpha * self.reward_prediction_error(state,action, reward, next_state, terminal) for action in self.action_space]\n",
        "                ) for possible_state in self.state_space])\n",
        "\n",
        "    def reward_prediction_error(self, state, action, reward, next_state, terminal):\n",
        "        if terminal:\n",
        "            return reward - self.q_table[state, action]\n",
        "        \n",
        "        next_action = self.policy(next_state)\n",
        "        return reward + self.gamma * self.q_table[next_state, next_action] - self.q_table[state, action]\n",
        "            \n",
        "    def update_beliefs(self, state, action, reward, next_state, terminal):\n",
        "        self.update_transition_model(state, action, next_state, terminal)\n",
        "        self.update_q_table(state, action, reward, next_state, terminal)\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "# one agent who can have different evaluation algos / policies / models?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from datetime import datetime\n",
        "\n",
        "# simulate data \n",
        "# (for now from randome agent, as test the environment and task implementation)\n",
        "def simulate_tow_step_task(env:TowStepEnv, agent=None, trails=200, policy_method=\"epsilon-greedy\"):\n",
        "    env.reset()\n",
        "    task_data = {}\n",
        "    \n",
        "    sd_for_random_walk = 0.025\n",
        "    time_step = 0\n",
        "    while time_step < trails:\n",
        "        # first stage choice\n",
        "        terminal = False\n",
        "        while not terminal:\n",
        "            current_state = env.state\n",
        "            if agent:\n",
        "                action = agent.policy(env.state, method=policy_method)\n",
        "            else: # if no agent is given -> random action\n",
        "                action = np.random.choice(env.action_space)\n",
        "\n",
        "            next_state, reward, terminal, info = env.step(action)\n",
        "            \n",
        "            if agent:\n",
        "                agent.update_beliefs(current_state, action, reward, next_state, terminal)\n",
        "            \n",
        "        task_data[time_step] = info\n",
        "        env.reset()\n",
        "        new_reward_prob_matrix = random_walk_gaussian(env.reward_prob_matrix, sd_for_random_walk)\n",
        "        env.set_reward_probabilities(new_reward_prob_matrix)\n",
        "        time_step += 1\n",
        "\n",
        "    return task_data\n",
        "\n",
        "def random_walk_gaussian(prob, sd, min_prob=0, max_prob=1):\n",
        "    new_prob = prob + np.random.normal(scale = sd, size=np.shape(prob))\n",
        "    new_prob = np.clip(new_prob, min_prob, max_prob)\n",
        "    return new_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "qtable:\n",
            " [[0.53557562 0.31877775]\n",
            " [0.66426142 0.14739877]\n",
            " [0.2557607  0.081     ]]\n",
            "transition model for relevant states-action:\n",
            " [[0.         0.72955975 0.27044025]\n",
            " [0.         0.34146341 0.65853659]]\n",
            "transition model for other states-actions:\n",
            " [[[0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n"
          ]
        }
      ],
      "source": [
        "# simulate the task\n",
        "# agent = RandomAgent(action_space=TowStepEnv.action_space, state_space=TowStepEnv.state_space)\n",
        "# agent = AgentModelFree(action_space=TowStepEnv.action_space, state_space=TowStepEnv.state_space)\n",
        "agent = AgentModelBased(action_space=TowStepEnv.action_space, state_space=TowStepEnv.state_space)\n",
        "env = TowStepEnv()\n",
        "task_data = simulate_tow_step_task(env, agent, trails=200)\n",
        "\n",
        "# (state, action) -> reward\n",
        "print(\"qtable:\\n\", agent.q_table)\n",
        "\n",
        "# (state, action, new state) -> transition probability\n",
        "if hasattr(agent, \"transition_model\"):\n",
        "    # only the relevant transition probabilities should be non-zero, all others should be zero\n",
        "    print(\"transition model for relevant states-action:\\n\", agent.transition_model[0])\n",
        "    print(\"transition model for other states-actions:\\n\", agent.transition_model[1:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "common transitions percentage: 71.5 %\n",
            "rewarded trails percentage: 41.5 %\n",
            "transition prob. from state 0 action 0 to state 1: 72.95597484276729 %\n",
            "transition prob. from state 0 action 1 to state 2: 65.85365853658537 %\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>State</th>\n",
              "      <th>Counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   State  Counts\n",
              "0      1     130\n",
              "1      2      70"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Action Stage 1</th>\n",
              "      <th>Counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Action Stage 1  Counts\n",
              "0               0     159\n",
              "1               1      41"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>common_transition</th>\n",
              "      <th>state_transition_to</th>\n",
              "      <th>reward_stage_1</th>\n",
              "      <th>action_stage_1</th>\n",
              "      <th>reward_stage_2</th>\n",
              "      <th>action_stage_2</th>\n",
              "      <th>reward_probabilities</th>\n",
              "      <th>trail_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.28926673057756686, 0.377241230728...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.29231358747512753, 0.377822222932...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 0.0, 0.2799033908597144, 0.3914184289877...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.3245719080131001, 0.3928764692649...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.3138006775248742, 0.4015376240067...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.5261839688961829, 0.3543094191420...</td>\n",
              "      <td>195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.4729856870751432, 0.3639637265564...</td>\n",
              "      <td>196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.4840573038940689, 0.3955094164030...</td>\n",
              "      <td>197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.4473081726634353, 0.4026836114425...</td>\n",
              "      <td>198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 0.4720768593737752, 0.3842427204879...</td>\n",
              "      <td>199</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     common_transition  state_transition_to  reward_stage_1  action_stage_1  \\\n",
              "0                 True                    1               0               0   \n",
              "1                 True                    1               0               0   \n",
              "2                 True                    1               0               0   \n",
              "3                 True                    1               0               0   \n",
              "4                 True                    1               0               0   \n",
              "..                 ...                  ...             ...             ...   \n",
              "195               True                    1               0               0   \n",
              "196              False                    2               0               0   \n",
              "197               True                    1               0               0   \n",
              "198               True                    1               0               0   \n",
              "199               True                    1               0               0   \n",
              "\n",
              "     reward_stage_2  action_stage_2  \\\n",
              "0                 0               0   \n",
              "1                 0               0   \n",
              "2                 0               1   \n",
              "3                 1               0   \n",
              "4                 0               0   \n",
              "..              ...             ...   \n",
              "195               1               0   \n",
              "196               0               0   \n",
              "197               1               0   \n",
              "198               1               0   \n",
              "199               1               0   \n",
              "\n",
              "                                  reward_probabilities  trail_index  \n",
              "0    [0.0, 0.0, 0.28926673057756686, 0.377241230728...            0  \n",
              "1    [0.0, 0.0, 0.29231358747512753, 0.377822222932...            1  \n",
              "2    [0.0, 0.0, 0.2799033908597144, 0.3914184289877...            2  \n",
              "3    [0.0, 0.0, 0.3245719080131001, 0.3928764692649...            3  \n",
              "4    [0.0, 0.0, 0.3138006775248742, 0.4015376240067...            4  \n",
              "..                                                 ...          ...  \n",
              "195  [0.0, 0.0, 0.5261839688961829, 0.3543094191420...          195  \n",
              "196  [0.0, 0.0, 0.4729856870751432, 0.3639637265564...          196  \n",
              "197  [0.0, 0.0, 0.4840573038940689, 0.3955094164030...          197  \n",
              "198  [0.0, 0.0, 0.4473081726634353, 0.4026836114425...          198  \n",
              "199  [0.0, 0.0, 0.4720768593737752, 0.3842427204879...          199  \n",
              "\n",
              "[200 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# convert the data to a dataframe\n",
        "task_df = pd.DataFrame.from_dict(task_data, orient='index')\n",
        "task_df['trail_index'] = task_df.index\n",
        "\n",
        "# save the data to a csv file\n",
        "time_identifier = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "task_df.to_csv(f\"data/task_data_{time_identifier}.csv\", index=False)\n",
        "\n",
        "# print some statistics \n",
        "# TODO stay probabilities?\n",
        "print(\"common transitions percentage:\", np.mean(task_df[\"common_transition\"])*100, \"%\")\n",
        "print(\"rewarded trails percentage:\", np.mean(task_df[\"reward_stage_2\"] > 0)*100, \"%\")\n",
        "print(\"transition prob. from state 0 action 0 to state 1:\", np.mean(task_df[task_df[\"action_stage_1\"] == 0][\"state_transition_to\"] == 1)*100, \"%\")\n",
        "print(\"transition prob. from state 0 action 1 to state 2:\", np.mean(task_df[task_df[\"action_stage_1\"] == 1][\"state_transition_to\"] == 2)*100, \"%\")\n",
        "\n",
        "visited_states_counts_df = task_df['state_transition_to'].value_counts().reset_index()\n",
        "visited_states_counts_df.columns = ['State', 'Counts']\n",
        "display(visited_states_counts_df)\n",
        "\n",
        "action_first_stage_counts_df = task_df['action_stage_1'].value_counts().reset_index()\n",
        "action_first_stage_counts_df.columns = ['Action Stage 1', 'Counts']\n",
        "display(action_first_stage_counts_df)\n",
        "\n",
        "\n",
        "display(task_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stepOne_Param</th>\n",
              "      <th>stepOneTwo_Param</th>\n",
              "      <th>stepTwoTwo_Param</th>\n",
              "      <th>rewards_Param</th>\n",
              "      <th>stepOneChoice</th>\n",
              "      <th>stepTwoChoice</th>\n",
              "      <th>reward</th>\n",
              "      <th>rewardProbabilities</th>\n",
              "      <th>isHighProbOne</th>\n",
              "      <th>isHighProbTwo</th>\n",
              "      <th>trial_type</th>\n",
              "      <th>trial_index</th>\n",
              "      <th>time_elapsed</th>\n",
              "      <th>internal_node_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[3,2]</td>\n",
              "      <td>[true,true,false,false]</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>[0.42698213416971215,0.4627012728182879,0.3133...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>dawsTwoStep</td>\n",
              "      <td>0</td>\n",
              "      <td>199187</td>\n",
              "      <td>0.0-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0,1]</td>\n",
              "      <td>[0,1]</td>\n",
              "      <td>[2,3]</td>\n",
              "      <td>[true,false,true,true]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>[0.41278756730835614,0.488285663822515,0.30118...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>dawsTwoStep</td>\n",
              "      <td>1</td>\n",
              "      <td>201995</td>\n",
              "      <td>0.0-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0,1]</td>\n",
              "      <td>[2,3]</td>\n",
              "      <td>[2,3]</td>\n",
              "      <td>[false,true,false,false]</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>[0.4161663996959537,0.45233292242685597,0.25,0...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>dawsTwoStep</td>\n",
              "      <td>2</td>\n",
              "      <td>204233</td>\n",
              "      <td>0.0-2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[0,1]</td>\n",
              "      <td>[true,true,true,false]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>[0.3682809695357653,0.485058262290801,0.255683...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>dawsTwoStep</td>\n",
              "      <td>3</td>\n",
              "      <td>206293</td>\n",
              "      <td>0.0-3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[0,1]</td>\n",
              "      <td>[false,false,false,false]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>[0.35795686167755414,0.4919457226516181,0.2563...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>dawsTwoStep</td>\n",
              "      <td>4</td>\n",
              "      <td>208362</td>\n",
              "      <td>0.0-4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[2,3]</td>\n",
              "      <td>[false,true,true,true]</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>[0.5824104590833694,0.75,0.4648248909993357,0....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>dawsTwoStep</td>\n",
              "      <td>195</td>\n",
              "      <td>632885</td>\n",
              "      <td>0.0-195.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[3,2]</td>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[true,false,true,true]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>[0.5536965041858476,0.7413805858314367,0.47677...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>dawsTwoStep</td>\n",
              "      <td>196</td>\n",
              "      <td>634934</td>\n",
              "      <td>0.0-196.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>[0,1]</td>\n",
              "      <td>[2,3]</td>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[true,true,false,false]</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>[0.5564089642172411,0.71873663019763,0.4886982...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>dawsTwoStep</td>\n",
              "      <td>197</td>\n",
              "      <td>641077</td>\n",
              "      <td>0.0-197.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[2,3]</td>\n",
              "      <td>[3,2]</td>\n",
              "      <td>[false,true,true,true]</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "      <td>[0.600021338675008,0.7060795212734965,0.486364...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>dawsTwoStep</td>\n",
              "      <td>198</td>\n",
              "      <td>643497</td>\n",
              "      <td>0.0-198.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>[0,1]</td>\n",
              "      <td>[0,1]</td>\n",
              "      <td>[1,0]</td>\n",
              "      <td>[false,true,false,true]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>[0.6236615561784724,0.6755664415488586,0.48905...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>dawsTwoStep</td>\n",
              "      <td>199</td>\n",
              "      <td>645977</td>\n",
              "      <td>0.0-199.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows Ã— 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    stepOne_Param stepOneTwo_Param stepTwoTwo_Param  \\\n",
              "0           [1,0]            [1,0]            [3,2]   \n",
              "1           [0,1]            [0,1]            [2,3]   \n",
              "2           [0,1]            [2,3]            [2,3]   \n",
              "3           [1,0]            [1,0]            [0,1]   \n",
              "4           [1,0]            [1,0]            [0,1]   \n",
              "..            ...              ...              ...   \n",
              "195         [1,0]            [1,0]            [2,3]   \n",
              "196         [1,0]            [3,2]            [1,0]   \n",
              "197         [0,1]            [2,3]            [1,0]   \n",
              "198         [1,0]            [2,3]            [3,2]   \n",
              "199         [0,1]            [0,1]            [1,0]   \n",
              "\n",
              "                 rewards_Param  stepOneChoice  stepTwoChoice  reward  \\\n",
              "0      [true,true,false,false]              1              3   False   \n",
              "1       [true,false,true,true]              0              0    True   \n",
              "2     [false,true,false,false]              0              2   False   \n",
              "3       [true,true,true,false]              1              0    True   \n",
              "4    [false,false,false,false]              1              0   False   \n",
              "..                         ...            ...            ...     ...   \n",
              "195     [false,true,true,true]              1              2    True   \n",
              "196     [true,false,true,true]              1              0    True   \n",
              "197    [true,true,false,false]              0              2   False   \n",
              "198     [false,true,true,true]              1              3    True   \n",
              "199    [false,true,false,true]              0              1    True   \n",
              "\n",
              "                                   rewardProbabilities  isHighProbOne  \\\n",
              "0    [0.42698213416971215,0.4627012728182879,0.3133...           True   \n",
              "1    [0.41278756730835614,0.488285663822515,0.30118...           True   \n",
              "2    [0.4161663996959537,0.45233292242685597,0.25,0...          False   \n",
              "3    [0.3682809695357653,0.485058262290801,0.255683...           True   \n",
              "4    [0.35795686167755414,0.4919457226516181,0.2563...           True   \n",
              "..                                                 ...            ...   \n",
              "195  [0.5824104590833694,0.75,0.4648248909993357,0....           True   \n",
              "196  [0.5536965041858476,0.7413805858314367,0.47677...          False   \n",
              "197  [0.5564089642172411,0.71873663019763,0.4886982...          False   \n",
              "198  [0.600021338675008,0.7060795212734965,0.486364...          False   \n",
              "199  [0.6236615561784724,0.6755664415488586,0.48905...           True   \n",
              "\n",
              "     isHighProbTwo   trial_type  trial_index  time_elapsed internal_node_id  \n",
              "0             True  dawsTwoStep            0        199187          0.0-0.0  \n",
              "1             True  dawsTwoStep            1        201995          0.0-1.0  \n",
              "2             True  dawsTwoStep            2        204233          0.0-2.0  \n",
              "3            False  dawsTwoStep            3        206293          0.0-3.0  \n",
              "4            False  dawsTwoStep            4        208362          0.0-4.0  \n",
              "..             ...          ...          ...           ...              ...  \n",
              "195           True  dawsTwoStep          195        632885        0.0-195.0  \n",
              "196          False  dawsTwoStep          196        634934        0.0-196.0  \n",
              "197          False  dawsTwoStep          197        641077        0.0-197.0  \n",
              "198           True  dawsTwoStep          198        643497        0.0-198.0  \n",
              "199          False  dawsTwoStep          199        645977        0.0-199.0  \n",
              "\n",
              "[200 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "common transitions percentage: 71.5 %\n",
            "rewarded trails percentage: 50.5 %\n"
          ]
        }
      ],
      "source": [
        "# load and inspect human data\n",
        "human_data = pd.read_csv(\"data/experiment_data_andrei.csv\")\n",
        "display(human_data)\n",
        "\n",
        "# print some statistics\n",
        "common_transitions_percentage = np.mean(\n",
        "    np.where(human_data[\"stepOneChoice\"] == 0, human_data[\"isHighProbOne\"], human_data[\"isHighProbTwo\"])\n",
        ") * 100\n",
        "print(\"common transitions percentage:\", common_transitions_percentage, \"%\")\n",
        "print(\"rewarded trails percentage:\", np.mean(human_data[\"reward\"] > 0)*100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'common_transition': True, 'state_transition_to': 2, 'reward_stage_1': 0, 'action_stage_1': 1, 'reward_stage_2': 0, 'action_stage_2': 0, 'reward_probabilities': array([0.        , 0.        , 0.73582736, 0.52061943, 0.40996409,\n",
            "       0.50099739])}\n"
          ]
        }
      ],
      "source": [
        "# simple one run test\n",
        "# np.random.seed(0)\n",
        "env = TowStepEnv()\n",
        "\n",
        "terminal = False\n",
        "while not terminal:\n",
        "    action = np.random.choice([0,1])\n",
        "    s, r, terminal, info = env.step(action)\n",
        "\n",
        "print(env.info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twTHrAfL2PC_"
      },
      "source": [
        "## Model Simulation *(3 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Simulate data from both models for a single set of parameters. The simulation should mimic the experiment you are trying to model. *(2 points)*\n",
        "\n",
        "*   Plot the simulated behavior of both models. *(1 point)*\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIk8efpKv-m-"
      },
      "outputs": [],
      "source": [
        "# YOUR MODEL SIMULATION CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VxwTW9LwnvJ"
      },
      "source": [
        "## Parameter Fitting *(4 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Set up a suitable parameter search space *(1 point)*\n",
        "\n",
        "*   Implement a procedure to evaluate the fit of a model based on data *(2 points)*\n",
        "\n",
        "*   Implement a procedure for searching the parameter space. *(1 point)*\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5OKVYszx7tQ"
      },
      "outputs": [],
      "source": [
        "# YOUR PARAMETER FITTING CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueZgrw_ByFsF"
      },
      "source": [
        "## Parameter Recovery *(5 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Set up a suitable space of parameters relevant for parameter recovery *(1 point)*\n",
        "\n",
        "*   Use the functions above to generate behavior from a models, for a given set of (randomly sampled) parameters, and then fit the model to its generated data. Make sure to evaluate the parameter fit in a quantiative manner. *(3 points)*\n",
        "\n",
        "*   Plot the parameter recovery results for both models. *(1 point)*\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLdarAD8yXwN"
      },
      "outputs": [],
      "source": [
        "# YOUR PARAMETER RECOVERY CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naQNlDBfzjnG"
      },
      "source": [
        "## *Optional*: Model Recovery *(2 bonus points)*\n",
        "\n",
        "In this bonus exercise, you may examine model reovery. The bonus points count towards your total group project points. That is, you may accumlate up to 22 points in the practical part of the group project.\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIR51ujwziTM"
      },
      "outputs": [],
      "source": [
        "# YOUR MODEL RECOVERY CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q4dfJ7O0BpW"
      },
      "source": [
        "## Model Comparison *(5 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Load and (potentially) preprocess the experimental data. (1 point)\n",
        "\n",
        "*   Fit the two models to the data.  *(1 point)*\n",
        "\n",
        "*   Evaluate which model performs better, taking into account fit and model complexity. *(2 points)*\n",
        "\n",
        "*   Plot the behavior of the winning model against the data. *(1 point)**\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wAGI-kd1yRb"
      },
      "outputs": [],
      "source": [
        "# YOUR MODEL COMPARISON CODE GOES HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
